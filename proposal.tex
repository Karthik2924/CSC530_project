\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage[usenames,dvipsnames]{color}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{proposal.bib}
\title{%
	\Large Bees? DNA Motif Discovery with Alternating Global-Local Search  \\
	\large \; \\ - CSC 530: Group 2 Project Proposal -}
\author{Grant Billings and Karthik Sanka}
\date{09/29/2022}
\begin{document}
\maketitle
\section*{\large{Abbreviations}}
\textbf{(l, d)}: a planted motif of length \textit{l} with \textit{d} random changes; \textbf{DNA}: Deoxyribonucleic Acid; \textbf{HMC}: Hamiltonian Monte Carlo; \textbf{MEME}: Multiple Expectation Maximization for Motif Elicitation; \textbf{PSO}: Particle Swarm Optimization;
\section{Executive Summary}
\textcolor{cyan}{done!} \\
Living organisms have genomes that evolve randomly over time, with natural selection working to increase the frequency of functionally beneficial sequences over generations. Motifs are non-random nucleotide sequences that in many cases have been shown to have biological function in gene regulation. Detection of motifs in sets of sequences is challenging because random mutations make exact matching of sequences ineffective, and brute force methods are very slow. The most popular software package for motif discovery is MEME, which works well but slows down significantly if many query sequences are provided. Motif discovery across large data sets has become an important step in genome analysis. \textit{Software that can efficiently mine these sequences for motifs is needed.}  
  
Nature-inspired algorithms have promise for DNA motif discovery since they broadly allow for efficient exploration of potential motifs while allowing good solutions to learn from each other. We propose use of Particle Swarm Optimization with Hamiltonian Monte Carlo (PSO-HMC) in alternating cycles of global and local search to quickly find motifs. Our algorithm will be tuned using implanted motifs in simulated data, tested on previously characterized benchmarking data, and finally applied to discover new sequence motifs in a cotton promoter sequence dataset. Sensitivity, specificity, and running time will be used to compare performance between our software and other widely used alternatives. At the end of the semester, we will present our findings in comparison to other available software in a poster. We will also create an animation showing the algorithm running and share it upload it to Wikipedia so others can gain a visual intuition for how PSO-HMC works. \textit{Our work will contribute to the rapid characterization of large genomic datasets.}
\section{Abstract}
\textcolor{cyan}{done!} \\
Biologists are interested in detecting motifs from DNA sequencing data because of their role in gene expression and chromatin architecture.
The (l, d) planted motif problem is NP-complete, so heuristics are usually employed to find motifs. Non-probabilistic scoring functions for potential motifs and their positions in sequences are discrete, making the non-convex, non-smooth solution space very difficult to work with using traditional optimization techniques. Nature-inspired algorithms tend to excel in problems of this type due to the ability for the algorithm to exchange information on potential solutions. Here, we propose a novel method for motif discovery using 1) alternating rounds of Particle Swarm Optimization for efficient global exploration of the solution space; and 2) Hamiltonian Monte Carlo for detailed local search to avoid poor outcomes due to local optima. We will implement our algorithm in Julia, and benchmark on synthetic and real datasets. Key deliverables include a poster presentation, as well as release of a graphical representation of the algorithm and code into the public domain. We hope the speed and quality of the predicted motifs will help researchers generate hypotheses for motif sequences that can then be functionally validated through wet lab experiments.
\section{Prior Work}
\textcolor{blue}{KARTHIK} \\
Nature-inspired algorithms are elegant heuristic solutions to many of the most challenging computational problems in the sciences \cite{fister2013brief}. Particle swarm optimization (PSO) is one such algorithm, inspired by the behavior of cooperative populations of bees or birds that ``fly together''. The method is reviewed well by Banks et al 2007 \cite{banks2007review}. The main use case of PSO is for finding globally optimal or near-optimal solutions for problems with a non-convex score function, where there may be many local optima throughout the search space. 
  
PSO has been used multiple times to find solutions for the motif discovery problem \cite{hardin2005dna,lei2010particle,reddy2010planted,ge2019discovery}. For this proposal, Lei and Ruan, 2010 \cite{lei2010particle}, served as a \textbf{template paper} inspiring our current work. Their main contribution was in implementing PSO for the motif finding problem by modifying the standard algorithm to take discrete inputs, as well as using both consensus sequences and position weight matrices for scoring. Since PSO does not invoke a gradient calculation (or even require the function be continuous or differentiable), it is highly flexible, but makes individual particles unable to explore the local search space efficiently without combining PSO with an additional algorithm. Lei and Ruan note that a main weakness of PSO is the inability to escape from local optima, which they circumvent by occasionally shifting the motif start sites to see if a better scoring solution is nearby. In other works such as Hardin and Rouchka, 2005 \cite{hardin2005dna}, an expectation maximization step is used to search close to the particles to see if a better score can be discovered before the swarm step.

\cite{lei2010particle}(our template paper) makes a slight modification to the update rule which is usually defined in a continuous domain, as a summation of vectors. The modification is applied by using a combination of scaling factor, a weight function as well as a random function. This combination balances the algorithm and allows it to reach the optimum as well as explore the space. These updates are done iteratively till a fixed number of iterations are reached or the fitness value remains the same. PSO being a meta-heuristic algorithm suffers from the problem of bad initialization, so the algorithms is restarted from a random initialization several times to increase the probability of convergence.
The algorithm also makes another important contribution to the motif finding problem using PSO, they make available a choice to the user to enter motif length and gap length to customize the algorithm to find gapped motifs. 
\section{Project Description}
\subsection{Data}
\textcolor{red}{GRANT} \\
Planted motifs of size $l \in [5, 15]$ and $d \in [1, \lfloor \frac{l}{2} \rfloor]$, corresponding to at most half of the nucleotides in the motif being mutated, will be simulated using Julia code (we have already developed the code for simulation). Motifs will be planted into $3$ 1000bp-long sequences during development, but the program will be evaluated on sets of $100$ sequences. Planted motifs will be used for tuning the hyper-parameters in PSO and HMC.  
  
  The program will also be benchmarked using manually curated motif binding sites from the online database resource \texttt{footprintDB} \cite{sebastian2014footprintdb}.  

  The program will be applied to find motifs in the 1000 bp upstream of 314 Upland cotton fiber-specific discovered in Ando et al, 2021 \cite{ando2021lcm}. Genome sequences 1000 bp upstream of the start codon will be extracted from the v2.1 Upland cotton genome assembly \cite{chen2020genomic} using \texttt{samtools faidx} \cite{li2009sequence}.
\subsection{The Algorithm}
\textcolor{red}{GRANT} \\
Each particle is initialized with some random position and velocity, corresponding to a proposed solution to the problem and the direction of the next proposed solution to the problem if the particle were left unperturbed. The score function is evaluated at each particle, and the particle with the best current score is noted. Then, a second set of velocity vectors between each particle and the best particle are computed. The initial velocity vector and this second vector are then composed to determine the next position of each particle. This is accomplished by weighting the hyper-parameters ``inertia'' (how much each particle wants to keep going in its current direction) and ``social attraction'' (how much each particle wants to head towards the best particle). The theoretical idea is that the algorithm will end up spending more time near the global optimum, converging quickly and exploring the solution space little if the social attraction is weighted highly, and the opposite if inertia is weighted highly.  
  
Describe HMC here? \cite{betancourt2017conceptual}
\subsection{Implementation}
\textcolor{red}{GRANT} \\
The PSO-HMC algorithm for motif discovery will be implemented using \texttt{Julia 1.8} \cite{Julia-2017} within a  \texttt{Jupyter Notebook}  \cite{Kluyver2016jupyter}. Five main functions will need to be implemented:
\begin{itemize}
	\item \texttt{Score}: returns the score for a set of sequences, motif length, and the proposed starting positions of the motif in each sequence.
	\item \texttt{FindMotifs}: takes DNA sequences and the motif length as inputs. Returns the ten best consensus motif sequences and the motif start sites in each DNA sequence. Iterates through HMC and PSO until stopping criteria is met.
	\item \texttt{HMC}: searches the space nearby the particle's current position using Hamiltonian dynamics. Utilizes a discrete estimate of a line integral (based on a discrete version of finite differences) rather than the gradient to determine the trajectory across the solution space. Returns an estimate of the score density in the region surrounding the particle.
	\item \texttt{PSO}: augments the score density near each particle given the output of \texttt{HMC} for the particle and the position of the best particle. Works by adding more weight to the density in the direction of the highest scoring particle.
	\item \texttt{UpdateParticle!}: draws a new position for each particle. Sets each particle's new velocity by composing its current velocity with a vector in the direction of highest scoring particle. Must be fine-tuned based on inertia and social attraction hyper-parameters.
\end{itemize}
\subsection{Experiments}
\textcolor{red}{GRANT} \\
Computational experiments will be conducted to answer three questions:
\subsubsection{Question 1: How does PSO-HMC CPU and memory usage scale to length and number of input sequences?}
\subsubsection{Question 2: How many mismatches can PSO-HMC tolerate in detecting motifs?}
\subsubsection{Question 3: What is PSO-HMC performance when there are zero instances of the motif in some sequences?}
\subsection{Evaluation and Statistics}
\textcolor{blue}{KARTHIK} \\
\begin{itemize}
	\item \textbf{Sensitivity}
	\item \textbf{Specificity}
	\item \textbf{Running Time}
	\item \textbf{Memory Usage}
\end{itemize}
\subsection{Deliverables}
\textcolor{red}{GRANT} \\
\begin{itemize}
	\item \textbf{Poster} presentation to CSC 530 classmates
	\item \textbf{Graphics} demonstrating the algorithm
	\item \textbf{Code} on GitHub
\end{itemize}
\subsection{Anticipated Problems and Solutions}
\textcolor{blue}{KARTHIK} \\
\begin{itemize}
	\item Poor model convergence/long running time?
\end{itemize}
\pagebreak
\section{Timeline}
\textcolor{green}{todo!} \\
\pagebreak
\section{Appendix}
\textcolor{cyan}{done!}
\begin{algorithm}
\caption{Motif Detection with PSO-HMC}
\begin{algorithmic}
\ForAll{$\textrm{motif lengths } k \in k_{\textrm{min}}..k_{\textrm{max}}$}
	\Comment{repeat algorithm for each plausible motif length}
	\State{Initialize a set $M$ of particle position vectors and velocities $m$ containing $p$ particles in $\mathbb{Z}^{n}$}
	\State{Initialize a dictionary for the 10 best motif starting positions $M_{\textrm{best}}$ and their scores}
	\State{Initialize a vector $V$ for storing the scoring distribution information near each $m$}
	\State{$i \gets 1$}
	\While{not converged or $i <$ iteration limit}
		\Comment{search until all particles are very close}
		\ForAll{particles $m_{i} \in 1..p$}
			\Comment{do local search near each particle}
			\State{Evaluate the current score with $Score(m_{i})$}
			\Comment{score is hamming dist. against consensus}
			\If{$Score(m_{i}) > \min(M_{\textrm{best}})$}
				\State{Add the score and position $M_{\textrm{best}}[m_{i}] \gets  Score(m_{i})$}
				\Comment{store for update step and output}
			\EndIf
			\State{Initialize a dictionary $O$ for the $q$ motif starting positions and their scores}
			\ForAll{sampled particles $o_{j} \in 1..q$}
				\Comment{local search step}
				\State{Allow the particle to roll in the solution space near $m_{i}$}
				\State{Add the resulting motif starting positions and scores $O[o_{j}] \gets Score(o_{j})$}
				\If{$Score(o_{i}) > \min(M_{\textrm{best}})$}
					\State{Add the score and position $M_{\textrm{best}}[o_{i}] \gets  Score(o_{i})$}
					\Comment{store for update step and output}
				\EndIf
			\EndFor
			\State{$V[i] \gets O$}
			\Comment{store local search results for update step}
		\EndFor
		\ForAll{particles $m_{i} \in 1..p$}
			\Comment{global search step to pull particles towards best particle}
			\State{Use $V[i]$ and $\textrm{argmax}(M_{best})$ to propose a new position and direction for $m_{i}$}
			\State{$M_{i} \gets m_{i}$}
		\EndFor
		\State{$i \gets i + 1$}
	\EndWhile
	\State \Return{$M_{\textrm{best}}$}
\EndFor
\end{algorithmic}
\end{algorithm}
\pagebreak
\printbibliography
\end{document}

